{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mountain Car.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "1AobWix05oho"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SkRb1fIA_sN"
      },
      "source": [
        "#@title Import modules.\n",
        "## Version utilisant le tutoriel Ã  modifier\n",
        "\n",
        "\n",
        "!sudo apt-get install -y xvfb ffmpeg\n",
        "!pip install 'gym==0.10.11'\n",
        "!pip install imageio\n",
        "!pip install PILLOW\n",
        "!pip install 'pyglet==1.3.2'\n",
        "!pip install pyvirtualdisplay\n",
        "\n",
        "!pip install dm-acme\n",
        "!pip install dm-acme[reverb]\n",
        "!pip install dm-acme[tf]\n",
        "!pip install dm-acme[envs]\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "\n",
        "#@title Import modules.\n",
        "#python3\n",
        "\n",
        "import copy\n",
        "import pyvirtualdisplay\n",
        "import imageio \n",
        "import base64\n",
        "import IPython\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "from acme import environment_loop\n",
        "from acme.tf import networks\n",
        "from acme.adders import reverb as adders\n",
        "from acme.agents.tf import actors as actors\n",
        "from acme.datasets import reverb as datasets\n",
        "from acme.wrappers import gym_wrapper\n",
        "from acme import specs\n",
        "from acme import wrappers\n",
        "from acme.agents.tf import d4pg\n",
        "from acme.agents import agent\n",
        "from acme.tf import utils as tf2_utils\n",
        "from acme.utils import loggers\n",
        "\n",
        "import gym \n",
        "import dm_env\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import reverb\n",
        "import sonnet as snt\n",
        "import tensorflow as tf\n",
        "\n",
        "# Import dm_control if it exists.\n",
        "try:\n",
        "  from dm_control import suite\n",
        "except (ModuleNotFoundError, OSError):\n",
        "  pass\n",
        "\n",
        "# Set up a virtual display for rendering OpenAI gym environments.\n",
        "display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Si2uhFP_EGo3"
      },
      "source": [
        "environment_name = 'gym_mountaincar'  # @param ['dm_cartpole', 'gym_mountaincar']\n",
        "# task_name = 'balance'  # @param ['swingup', 'balance']\n",
        "\n",
        "def make_environment(domain_name='cartpole', task='balance'):\n",
        "  env = suite.load(domain_name, task)\n",
        "  env = wrappers.SinglePrecisionWrapper(env)\n",
        "  return env\n",
        "\n",
        "if 'dm_cartpole' in environment_name:\n",
        "  environment = make_environment('cartpole')\n",
        "  def render(env):\n",
        "    return env._physics.render(camera_id=0)  #pylint: disable=protected-access\n",
        "\n",
        "elif 'gym_mountaincar' in environment_name:\n",
        "  environment = gym_wrapper.GymWrapper(gym.make('MountainCarContinuous-v0'))\n",
        "  environment = wrappers.SinglePrecisionWrapper(environment)\n",
        "  def render(env):\n",
        "    return env.environment.render(mode='rgb_array')\n",
        "else:\n",
        "  raise ValueError('Unknown environment: {}.'.format(environment_name))\n",
        "\n",
        "# Show the frame.\n",
        "frame = render(environment)\n",
        "plt.imshow(frame)\n",
        "plt.axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4Pgd4_ylW4u"
      },
      "source": [
        "# Environment spec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A97d8IRO4oyD"
      },
      "source": [
        "environment_spec = specs.make_environment_spec(environment)\n",
        "\n",
        "print('actions:\\n', environment_spec.actions, '\\n')\n",
        "print('observations:\\n', environment_spec.observations, '\\n')\n",
        "print('rewards:\\n', environment_spec.rewards, '\\n')\n",
        "print('discounts:\\n', environment_spec.discounts, '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggfxsr5G45sr"
      },
      "source": [
        "# Build the policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8miJ5mMp47lm"
      },
      "source": [
        "# Calculate how big the last layer should be based on total # of actions.\n",
        "action_spec = environment_spec.actions\n",
        "action_size = np.prod(action_spec.shape, dtype=int)\n",
        "exploration_sigma = 0.3\n",
        "\n",
        "# In order the following modules:\n",
        "# 1. Flatten the observations to be [B, ...] where B is the batch dimension.\n",
        "# 2. Define a simple MLP which is the guts of this policy.\n",
        "# 3. Make sure the output action matches the spec of the actions.\n",
        "policy_modules = [\n",
        "    tf2_utils.batch_concat,\n",
        "    networks.LayerNormMLP(layer_sizes=(300, 200, action_size)),\n",
        "    networks.TanhToSpec(spec=environment_spec.actions)]\n",
        "\n",
        "policy_network = snt.Sequential(policy_modules)\n",
        "\n",
        "# We will also create a version of this policy that uses exploratory noise.\n",
        "behavior_network = snt.Sequential(\n",
        "    policy_modules + [networks.ClippedGaussian(exploration_sigma),\n",
        "                      networks.ClipToSpec(action_spec)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gH6LKWLb5Bsi"
      },
      "source": [
        "# Create the actor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mx0S80M5FZ4"
      },
      "source": [
        "actor = actors.FeedForwardActor(policy_network)\n",
        "[method_or_attr for method_or_attr in dir(actor)  # pylint: disable=expression-not-assigned\n",
        " if not method_or_attr.startswith('_')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WOWTt6n5MOP"
      },
      "source": [
        "# Evaluate the random actor's policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjgM8AGb5PfR"
      },
      "source": [
        "def display_video(frames, filename='temp.mp4'):\n",
        "  \"\"\"Save and display video.\"\"\"\n",
        "  # Write video\n",
        "  with imageio.get_writer(filename, fps=60) as video:\n",
        "    for frame in frames:\n",
        "      video.append_data(frame)\n",
        "  # Read video and display the video\n",
        "  video = open(filename, 'rb').read()\n",
        "  b64_video = base64.b64encode(video)\n",
        "  video_tag = ('<video  width=\"320\" height=\"240\" controls alt=\"test\" '\n",
        "               'src=\"data:video/mp4;base64,{0}\">').format(b64_video.decode())\n",
        "  return IPython.display.HTML(video_tag)\n",
        "\n",
        "\n",
        "# Run the actor in the environment for desired number of steps.\n",
        "frames = []\n",
        "num_steps = 500\n",
        "timestep = environment.reset()\n",
        "\n",
        "for _ in range(num_steps):\n",
        "  frames.append(render(environment))\n",
        "  action = actor.select_action(timestep.observation)\n",
        "  timestep = environment.step(action)\n",
        "\n",
        "# Save video of the behaviour.\n",
        "display_video(np.array(frames))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AobWix05oho"
      },
      "source": [
        "# Storing actor experiences in a replay buffer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFoBFdDK5ptF"
      },
      "source": [
        "# Create a table with the following attributes:\n",
        "# 1. when replay is full we remove the oldest entries first.\n",
        "# 2. to sample from replay we will do so uniformly at random.\n",
        "# 3. before allowing sampling to proceed we make sure there is at least\n",
        "#    one sample in the replay table.\n",
        "# 4. we use a default table name so we don't have to repeat it many times below;\n",
        "#    if we left this off we'd need to feed it into adders/actors/etc. below.\n",
        "replay_buffer = reverb.Table(\n",
        "    name=adders.DEFAULT_PRIORITY_TABLE,\n",
        "    max_size=1000000,\n",
        "    remover=reverb.selectors.Fifo(),\n",
        "    sampler=reverb.selectors.Uniform(),\n",
        "    rate_limiter=reverb.rate_limiters.MinSize(min_size_to_sample=1))\n",
        "\n",
        "# Get the server and address so we can give it to the modules such as our actor\n",
        "# that will interact with the replay buffer.\n",
        "replay_server = reverb.Server([replay_buffer], port=None)\n",
        "replay_server_address = 'localhost:%d' % replay_server.port\n",
        "\n",
        "\n",
        "# Create a 5-step transition adder where in between those steps a discount of\n",
        "# 0.99 is used (which should be the same discount used for learning).\n",
        "adder = adders.NStepTransitionAdder(\n",
        "    client=reverb.Client(replay_server_address),\n",
        "    n_step=5,\n",
        "    discount=0.99)\n",
        "\n",
        "num_episodes = 2  #@param\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "  timestep = environment.reset()\n",
        "  adder.add_first(timestep)\n",
        "\n",
        "  while not timestep.last():\n",
        "    action = actor.select_action(timestep.observation)\n",
        "    timestep = environment.step(action)\n",
        "    adder.add(action=action, next_timestep=timestep)\n",
        "\n",
        "actor = actors.FeedForwardActor(policy_network=behavior_network, adder=adder)\n",
        "num_episodes = 2  #@param\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "  timestep = environment.reset()\n",
        "  actor.observe_first(timestep)  # Note: observe_first.\n",
        "\n",
        "  while not timestep.last():\n",
        "    action = actor.select_action(timestep.observation)\n",
        "    timestep = environment.step(action)\n",
        "    actor.observe(action=action, next_timestep=timestep)  # Note: observe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4nzsE406CVZ"
      },
      "source": [
        "# Learning from experience"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMgxn6HW6Fbn"
      },
      "source": [
        "# This connects to the created reverb server; also note that we use a transition\n",
        "# adder above so we'll tell the dataset function that so that it knows the type\n",
        "# of data that's coming out.\n",
        "dataset = datasets.make_dataset(\n",
        "    server_address=replay_server_address,\n",
        "    batch_size=256,\n",
        "    environment_spec=environment_spec,\n",
        "    transition_adder=True)\n",
        "\n",
        "critic_network = snt.Sequential([\n",
        "    networks.CriticMultiplexer(\n",
        "        observation_network=tf2_utils.batch_concat,\n",
        "        action_network=tf.identity,\n",
        "        critic_network=networks.LayerNormMLP(\n",
        "            layer_sizes=(400, 300),\n",
        "            activate_final=True)),\n",
        "    # Value-head gives a 51-atomed delta distribution over state-action values.\n",
        "    networks.DiscreteValuedHead(vmin=-150., vmax=150., num_atoms=51)])\n",
        "\n",
        "# Create the target networks\n",
        "target_policy_network = copy.deepcopy(policy_network)\n",
        "target_critic_network = copy.deepcopy(critic_network)\n",
        "\n",
        "# We must create the variables in the networks before passing them to learner.\n",
        "tf2_utils.create_variables(network=policy_network,\n",
        "                           input_spec=[environment_spec.observations])\n",
        "tf2_utils.create_variables(network=critic_network,\n",
        "                           input_spec=[environment_spec.observations,\n",
        "                                       environment_spec.actions])\n",
        "tf2_utils.create_variables(network=target_policy_network,\n",
        "                           input_spec=[environment_spec.observations])\n",
        "tf2_utils.create_variables(network=target_critic_network,\n",
        "                           input_spec=[environment_spec.observations,\n",
        "                                       environment_spec.actions])\n",
        "\n",
        "learner = d4pg.D4PGLearner(policy_network=policy_network,\n",
        "                           critic_network=critic_network,\n",
        "                           target_policy_network=target_policy_network,\n",
        "                           target_critic_network=target_critic_network,\n",
        "                           dataset=dataset,\n",
        "                           discount=0.99,\n",
        "                           target_update_period=100,\n",
        "                           policy_optimizer=snt.optimizers.Adam(1e-4),\n",
        "                           critic_optimizer=snt.optimizers.Adam(1e-4),\n",
        "                           # Log learner updates to console every 10 seconds.\n",
        "                           logger=loggers.TerminalLogger(time_delta=10.),\n",
        "                           checkpoint=False)\n",
        "\n",
        "[method_or_attr for method_or_attr in dir(learner)  # pylint: disable=expression-not-assigned\n",
        " if not method_or_attr.startswith('_')]\n",
        "learner.step()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVfJak_j6gCG"
      },
      "source": [
        "# Training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDpQ8frY6hwv"
      },
      "source": [
        "adder.reset()\n",
        "\n",
        "num_training_episodes =  10 # @param {type: \"integer\"}\n",
        "min_actor_steps_before_learning = 1000  # @param {type: \"integer\"}\n",
        "num_actor_steps_per_iteration =   100 # @param {type: \"integer\"}\n",
        "num_learner_steps_per_iteration = 1  # @param {type: \"integer\"}\n",
        "\n",
        "learner_steps_taken = 0\n",
        "actor_steps_taken = 0\n",
        "episode_reward=[]\n",
        "episode_liste=[]\n",
        "\n",
        "\n",
        "for episode in range(num_training_episodes):\n",
        "  \n",
        "  timestep = environment.reset()\n",
        "  actor.observe_first(timestep)\n",
        "  episode_return = 0\n",
        "\n",
        "\n",
        "  while not timestep.last():\n",
        "    # Get an action from the agent and step in the environment.\n",
        "    action = actor.select_action(timestep.observation)\n",
        "    next_timestep = environment.step(action)\n",
        "\n",
        "    # Record the transition.\n",
        "    actor.observe(action=action, next_timestep=next_timestep)\n",
        "\n",
        "    # Book-keeping.\n",
        "    episode_return += next_timestep.reward\n",
        "    actor_steps_taken += 1\n",
        "    timestep = next_timestep\n",
        "\n",
        "\n",
        "    # See if we have some learning to do.\n",
        "    if (actor_steps_taken >= min_actor_steps_before_learning and\n",
        "        actor_steps_taken % num_actor_steps_per_iteration == 0):\n",
        "      # Learn.\n",
        "      for learner_step in range(num_learner_steps_per_iteration):\n",
        "        learner.step()\n",
        "      learner_steps_taken += num_learner_steps_per_iteration    \n",
        "\n",
        "\n",
        "  # Log quantities.\n",
        "  print('Episode: %d | Return: %f | Learner steps: %d | Actor steps: %d'%(\n",
        "      episode, episode_return, learner_steps_taken, actor_steps_taken))\n",
        "  episode_liste.append(episode)\n",
        "  episode_reward.append(episode_return)\n",
        "  print(episode_reward)\n",
        "\n",
        "  #episode_reward.append(episode_return)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d3DSALjrspJ"
      },
      "source": [
        "Introduce ACME Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACG-E9ov6pPD"
      },
      "source": [
        "d4pg_agent = agent.Agent(actor=actor,\n",
        "                         learner=learner,\n",
        "                         min_observations=1000,\n",
        "                         observations_per_step=8.)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wWE09_r7p9h"
      },
      "source": [
        "#First test : white/gaussian noise\n",
        "def white_noise(mu, sigma, current_env_spec):\n",
        "  dim = np.size(current_env_spec.observations)\n",
        "  noise = np.random.normal(mu, sigma, dim) #generate noise through random signal\n",
        "  return (noise)\n",
        "\n",
        "'''e=0,95'''\n",
        "print(d4pg_agent.select_action)\n",
        "def actionDecorator(og_function):\n",
        "  def nestedFunction(*args, **kwargs):\n",
        "    print(*args)\n",
        "    print(**kwargs)\n",
        "    E = white_noise(0,1,environment_spec)\n",
        "    args[0][0]=args[0][0]+E\n",
        "    args[0][1]=args[0][1]+E\n",
        "    #print(args[0][1])\n",
        "    results = og_function(*args, **kwargs)\n",
        "    return results\n",
        "  return nestedFunction\n",
        "\n",
        "d4pg_agent.select_action = actionDecorator(d4pg_agent.select_action)\n",
        "print(d4pg_agent.select_action)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXtNjucsrvxi"
      },
      "source": [
        "Training the full loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vM9EjGFPl4bw"
      },
      "source": [
        "#This may be necessary if any of the episodes were cancelled above.\n",
        "adder.reset()\n",
        "\n",
        "# We also want to make sure the logger doesn't write to disk because that can\n",
        "# cause issues in colab on occasion.\n",
        "logger = loggers.TerminalLogger(time_delta=10.)\n",
        "\n",
        "loop = environment_loop.EnvironmentLoop(environment, d4pg_agent,logger=logger)\n",
        "loop.run(num_episodes=200)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4v82rgq6vvf"
      },
      "source": [
        "# Evaluate D4PG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Py3xVFNY6x5o"
      },
      "source": [
        "# Run the actor in the environment for desired number of steps.\n",
        "frames = []\n",
        "num_steps = 1000\n",
        "timestep = environment.reset()\n",
        "\n",
        "for _ in range(num_steps):\n",
        "  frames.append(render(environment))\n",
        "  action = d4pg_agent.select_action(timestep.observation)\n",
        "  timestep = environment.step(action)\n",
        "\n",
        "# Save video of the behaviour.\n",
        "display_video(np.array(frames))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b-4KAW_8FxW"
      },
      "source": [
        "# Graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riQ9pcS08ISU"
      },
      "source": [
        "import numpy as np\n",
        "from statistics import NormalDist\n",
        "\n",
        "#test de normalitÃ© de Shapiro-Francia (renvoie ro(X,Z))\n",
        "def test_norm_SF(X):\n",
        "    n=X.shape[0]\n",
        "    muX=sum(X)/n\n",
        "    sigX=(sum(X**2)/n)-muX**2\n",
        "    X=sorted(X)\n",
        "    Fc=np.zeros(n)\n",
        "    for i in range(1,n+1):\n",
        "        Fc[i-1]=(i-0.375)/(n+0.25)\n",
        "    Z=np.zeros(n)\n",
        "    for i in range(n):\n",
        "        Z[i]=NormalDist().inv_cdf(Fc[i])\n",
        "    muZ=sum(Z)/n\n",
        "    sigZ=sum(Z**2)/n-muZ**2\n",
        "    c=0\n",
        "    for i in range(n):\n",
        "        c+=X[i]*Z[i]\n",
        "    return (c/n-muX*muZ)/np.sqrt(sigX*sigZ)\n",
        "\n",
        "#test d'indÃ©pendance (entrÃ©e : n(i,j), renvoie KH)\n",
        "def test_indep(N):\n",
        "    n=sum(sum(N))\n",
        "    l,m=N.shape\n",
        "    freq_marg_ligne=np.zeros(l)\n",
        "    freq_marg_col=np.zeros(m)\n",
        "    for i in range(l):\n",
        "        freq_marg_ligne[i]=sum(N[i,:])\n",
        "    for i in range(m):\n",
        "        freq_marg_col[i]=sum(N[:,i])\n",
        "    pp=np.zeros((l,m))\n",
        "    for i in range(l):\n",
        "        for j in range(m):\n",
        "            pp[i][j]=freq_marg_ligne[i]*freq_marg_col[j]\n",
        "    KH=0\n",
        "    for i in range(l):\n",
        "        for j in range(m):\n",
        "            KH+=((N[i][j]-pp[i][j]/n)**2)/(pp[i][j]/n)\n",
        "    return KH\n",
        "\n",
        "#test d'adÃ©quation Ã  un loi (renvoie KH)\n",
        "def test_adeq(N,P):\n",
        "    n=sum(N)\n",
        "    KH=0\n",
        "    A=n*P\n",
        "    for i in range(N.shape[0]):\n",
        "        KH+=((N[i]-A[i])**2)/A[i]\n",
        "    return KH\n",
        "\n",
        "#test d'homogÃ©nÃ©itÃ©  (renvoie KH)\n",
        "#--->utiliser test_indep"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}